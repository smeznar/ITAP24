{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaje 11: Manjkajoče vrednosti in neenakomerne porazdelitve ciljne spremenljivke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naloga 1: Manjkajoče vrednosti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V praksi se pogosto spopademo s podatki, ki vsebujejo manjkajoče vrednosti. V tej nalogi si bomo pogledali nekaj pristopov za spopadanje s takšnimi podatki, kot so:\n",
    "- uporaba napovednih modelov, ki se lahko spopade z njimi (npr. drevesa)\n",
    "- odstranitev problematičnih stolpcev\n",
    "- nadomestitev manjkajočih vrednosti v danem stolpcu s povprečjem znanih\n",
    "- napoved manjkajočih vrednosti v danem stolpcu s pomočjo modela\n",
    "\n",
    "Vse metode za delo z manjkajočimi vrednostmi vhodnih spremenljivk so primerne tudi za delo s podatki, ki jim manjkajo nekatere ciljne vrednosti, zato bodo nekatere funkcije sposobne odpraviti tudi manjkajoče vrednosti v ciljni spremenljivki (ali pa bi bile za to potrebne le majhne spremembe). Paziti je treba le (če to delamo pred razdelitvijo na učno in testno množico), da vrednosti ciljne spremenljivke iz testne ne uporabljamo.\n",
    "\n",
    "Na današnjih vajah bomo manjkajoče vrednosti označili z NA. Za zaznavanje NA-jev je koristna funkcija\n",
    "*is.na* (ter sorodna *anyNA*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sestavimo najprej podatkovno množico in jo razdelimo na učno in testno množico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "p = 0.4\n",
    "X = np.random.random((100, 10))\n",
    "y = X @ np.arange(11, 21) + X**2 @ np.arange(20, 10, -1)\n",
    "for i in range(100):\n",
    "    if np.random.random() < p:\n",
    "        X[i, np.random.randint(5, size=2)*2] = np.nan\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a: Najprej uporabimo odločitvena drevesa, ki se znajo brez dodatnih izboljšav spopasti z manjkajočimi podatki. Premisli zakaj je temu tako in preveri točnost odločitvenega drevesa na danih podatkih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree accuracy on test set; RMSE: 32.6140467605478\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Decision tree accuracy on test set; RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odločitvena drevesa primere razdelijo glede na to ali izpolnjujejo pogoj ali ne. Če je v podatkih torej manjkajoča vrednost, pogoj ni izpolnjen, torej gre primer v vejo, ki je namenjena primerom z neizpolnjenim pogojem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b: Drugi pristop je, da odstanimo stolpce oziroma vrstice, ki vsebujejo manjkajoče vrednosti. Preveri točnost modela, ki uporablja podatke, ki jim odstanimo stolpce z manjkajočimi vrednostmi oziroma vrstice z manjkajočimi vrednostmi. Pomagaš si lahko s funkcijama `numpy.isnan` in `numpy.any`. Opaziš kakšen problem pri tem pristopu?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree without NAN rows accuracy on test set; RMSE: 24.71219380385893\n",
      "Decision tree without NAN columns accuracy on test set; RMSE: 32.422976403296765\n"
     ]
    }
   ],
   "source": [
    "row_mask = np.isnan(X_train).any(axis=1)\n",
    "col_mask = np.isnan(X_train).any(axis=0)\n",
    "X_rows = X_train[row_mask]\n",
    "X_cols = X_train[:, col_mask]\n",
    "\n",
    "row_model = DecisionTreeRegressor().fit(X_rows, y_train[row_mask])\n",
    "y_pred = row_model.predict(X_test)\n",
    "print(f\"Decision tree without NAN rows accuracy on test set; RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")\n",
    "\n",
    "col_model = DecisionTreeRegressor().fit(X_cols, y_train)\n",
    "y_pred = col_model.predict(X_test[:, col_mask])\n",
    "print(f\"Decision tree without NAN columns accuracy on test set; RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pri uporabi tega pristopa lahko pride do več problemov. Kaj bomo napovedali za testne primere, ki vsebujejo NAN vrednosti, kaj če se NAN vrednost pojavi v stolpcu, ki v učni množici ni vseboval NAN vrednosti, kaj če vse vrstice/stolpci vsebujejo NAN vrednosti, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odstranjevanje značilk ali primerov je redko dobra izbira. Preprosta in kar dobra alternativa je manjkajočo vrednost zapolniti s povprečjem (aritmetičnim ali mediano) v primeru numeričnih spremenljivk, oziroma z najpogostejšo vrednostjo v primeru kategoričnih spremenljivk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c: Preveri točnost modela, ko manjkajoče vrednosti nadomestiš z aritmetičnim povprečjem in mediano. V praksi je mediana uporabna predvsem, ko imamo \"outlierje\", ki bi zelo pokvarili povprečje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree with mean imputer on test set; RMSE: 32.11413818201312\n",
      "Decision tree with median imputer on test set; RMSE: 32.60701772627695\n"
     ]
    }
   ],
   "source": [
    "mask = np.isnan(X_train)\n",
    "test_mask = np.isnan(X_test)\n",
    "mean_values = np.nanmean(X_train, axis=0)\n",
    "median_values = np.nanmedian(X_train, axis=0)\n",
    "X_train_mean = X_train.copy()\n",
    "X_train_median = X_train.copy()\n",
    "X_test_mean = X_test.copy()\n",
    "X_test_median = X_test.copy()\n",
    "\n",
    "for i in range(X_train_mean.shape[1]):\n",
    "    X_train_mean[mask[:, i], i] = mean_values[i]\n",
    "    X_train_median[mask[:, i], i] = median_values[i]\n",
    "    X_test_mean[test_mask[:, i], i] = mean_values[i]\n",
    "    X_test_median[test_mask[:, i], i] = median_values[i]\n",
    "\n",
    "mean_model = DecisionTreeRegressor().fit(X_train_mean, y_train)\n",
    "y_pred = mean_model.predict(X_test_mean)\n",
    "print(f\"Decision tree with mean imputer on test set; RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")\n",
    "\n",
    "median_model = DecisionTreeRegressor().fit(X_train_median, y_train)\n",
    "y_pred = median_model.predict(X_test_median)\n",
    "print(f\"Decision tree with median imputer on test set; RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najzahtevnejši pristop za obravnavo manjkajočih vrednosti je napovedovanje s pomočjo modela - za to se pogosto uporablja metoda najbližjih sosedov. Takšno imputacijo nam olajša sklearn-ov model KNNImputer. \n",
    "POZOR! Pri prejšnjih metodah ni bilo zares pomembno, pri imputaciji z modelom pa je: paziti moramo, da predprocesiranje treniramo na učni množici, njegove vrednosti pa potem uporabimo za predprocesiranje testne množice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.d: Z uporabo objekta KNNImputer dopolni manjkajoče vrednosti v podatkih in preveri točnost odločitvenega drevesa na testni množici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree with KNN imputer on test set; RMSE: 30.867584447653886\n"
     ]
    }
   ],
   "source": [
    "imputer = KNNImputer().fit(X_train)\n",
    "X_train_imputed = imputer.transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "imputed_model = DecisionTreeRegressor().fit(X_train_imputed, y_train)\n",
    "y_pred = imputed_model.predict(X_test_imputed)\n",
    "print(f\"Decision tree with KNN imputer on test set; RMSE: {np.sqrt(mean_squared_error(y_test, y_pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naloga 2: Neenakomerne porazdelitve ciljne spremenljivke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kadar je porazdelitev ciljne spremenljivke močno neenakomerna, lahko pri učenju modelov nastopijo težave. Ker bolj točne napovedi za pogosto opazovane vrednosti manjšajo napako, postanejo napovedi pogostih vrednosti ciljne spremenljivke bolj pomembne, in predsodek modela se poveča. Na primer, zelo majhen delež pozitivnih primerov pri dvojiški klasifikaciji lahko vodi do modela, ki vedno napove negativni razred. \n",
    "\n",
    "Eden od načinov za spopadanje z neenakomernimi porazdelitvami je previdna ročna konstrukcija funkcije napake, ki da večji poudarek manj zastopanim vrednostim. \n",
    "\n",
    "Drug popularen pristop za reševanje teh problem je uteženo vzorčenje podatkov. Ta pristop si bomo pogledali na tej vaji.\n",
    "\n",
    "Uporabili bomo podatkovno množico, ki vsebuje le okoli 5% pozitivnih primerov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"vaje11.npy\", allow_pickle=True)\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1].astype(bool)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.a: Podatki vsebujejo manjkajoče vrednosti. Z eno od zgornjih metod te vrednosti dopolni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train == \"?\"] = np.nan\n",
    "X_test[X_test == \"?\"] = np.nan\n",
    "X_train = np.array(X_train, dtype=np.float64)\n",
    "X_test = np.array(X_test, dtype=np.float64)\n",
    "X_test.astype(np.float64)\n",
    "mask = np.isnan(X_train)\n",
    "test_mask = np.isnan(X_test)\n",
    "median_values = np.nanmedian(X_train, axis=0)\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    X_train[mask[:, i], i] = median_values[i]\n",
    "    X_test[test_mask[:, i], i] = median_values[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najpreprostejši (in pogosto najboljši) metodi za uteženo vzorčenje sta:\n",
    "\n",
    "- podvzorčenje (undersampling ali downsampling): vzamemo vse primere manjšinskega razreda ter zgolj delež primerov večinskega razreda, tako da porazdelitev uravnovesimo\n",
    "\n",
    "- nadvzročenje (oversampling ali upsampling): podatkom dodamo neko število kopij primerov majšinskega razreda, tako da porazdelitev izenačimo\n",
    "\n",
    "Za uteževa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from imbalanced-learn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from imbalanced-learn) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from imbalanced-learn) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b: S pomočjo knjižnjice imabalanced-learn sestavi množico z nadvzorčenjem in podvzorčenjem. Preveri confusion matrix za model odločitvenega drevesa naučenega na navadni, nadvzorčeni in podvzorčeni učni množici.\n",
    "\n",
    "<details>\n",
    "  <summary>Namig:</summary>\n",
    "\n",
    "Za nadvzorčenje lahko uporabiš razred [imblearn.over_sampling.RandomOverSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html), za podvzorčenje pa razred [imblearn.under_sampling.RandomUnderSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html)\n",
    "   \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal data set\n",
      "[[195   8]\n",
      " [  8   0]]\n",
      "Undersampled data set\n",
      "[[160  43]\n",
      " [  6   2]]\n",
      "Oversampled data set\n",
      "[[198   5]\n",
      " [  8   0]]\n"
     ]
    }
   ],
   "source": [
    "rus = RandomUnderSampler()\n",
    "X_rus, y_rus = rus.fit_resample(X_train, y_train)\n",
    "ros = RandomOverSampler()\n",
    "X_ros, y_ros = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Normal data set\")\n",
    "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Undersampled data set\")\n",
    "model = DecisionTreeClassifier().fit(X_rus, y_rus)\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"Oversampled data set\")\n",
    "model = DecisionTreeClassifier().fit(X_ros, y_ros)\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.c: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sampled with SMOTE\n",
      "[[189  14]\n",
      " [  7   1]]\n"
     ]
    }
   ],
   "source": [
    "smote = SMOTE()\n",
    "X_smote, y_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Data set sampled with SMOTE\")\n",
    "model = DecisionTreeClassifier().fit(X_smote, y_smote)\n",
    "y_pred = model.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomembno: Vzorčenje (posebaj nadvzorčenje) moramo delati na učni množici (pri prečnem preverjanju v vsakem fold-u posebaj). V nasprotnem primeru lahko dodamo primere, ki se pojavijo v učni množici tudi v testno množici in tako umetno izboljšamo točnost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ITAP24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
