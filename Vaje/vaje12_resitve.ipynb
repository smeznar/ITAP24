{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ded8fd",
   "metadata": {},
   "source": [
    "# Vaje 12: Analiza besedil in vpetja podatkov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b76e602f9222128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:34:47.041946Z",
     "start_time": "2023-11-16T09:34:46.967739Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acaed9",
   "metadata": {},
   "source": [
    "Uporabljali bomo podatkovno množico recenzij filmov iz IMDB-ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5a728c063f9569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:34:52.668218Z",
     "start_time": "2023-11-16T09:34:46.992678Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no relation at all between Fortier an...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is a great. The plot is very true t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George P. Cosmatos' \"Rambo: First Blood Part I...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the process of trying to establish the audi...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeh, I know -- you're quivering with excitemen...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  There is no relation at all between Fortier an...   pos\n",
       "1  This movie is a great. The plot is very true t...   pos\n",
       "2  George P. Cosmatos' \"Rambo: First Blood Part I...   neg\n",
       "3  In the process of trying to establish the audi...   pos\n",
       "4  Yeh, I know -- you're quivering with excitemen...   neg"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Naloži podatkovno množico IMDb\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "\n",
    "# Izbremo 20000 podatkov iz učne množice\n",
    "data = pd.DataFrame(imdb_dataset['train'].shuffle(seed=42).select(range(20000)))\n",
    "\n",
    "# Ciljno vrednost spremenimo iz True v \"pos\" in iz False v \"neg\"\n",
    "data['label'] = data['label'].apply(lambda x: 'pos' if x else 'neg')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3159d682b2038a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T16:33:30.337646Z",
     "start_time": "2023-11-15T16:33:29.952699Z"
    },
    "collapsed": false
   },
   "source": [
    "# Predprocesiranje: Čiščenje podatkov\n",
    "\n",
    "Ena izmed najbolj pomembnih nalog pri analizi besedil je predprocesiranje podatkov. Besedila moramo torej spraviti v format, ki je ustrezen za algoritme strojnega učenja (torej matrike/tenzorje). Poglejmo si torej nekaj tehnik predprocesiranja, ki so skoraj obvezne, ko delamo z besedili:\n",
    "\n",
    "1. **Tokenizacija**: Čeprav na besede ponavadi gledamo kot na celoto, so le-te ponavadi sestavljene iz več delov, ki so skupni več besedam (naprimer: nogomet, rokomet, ...). Da bolje ujamemo te povezave med besedami, jih zato ponavadi razbijemo na manjše dele. Dodatno nam tokenizacija omogoča sestavljanje/kodiranje novih (še ne videnih) besed in manjšo množico vhodnih besed.\n",
    "\n",
    "2. **Lowercasing**: Da se izognemu razlikovanja med isto besedo, ko je napisana z veliko in malo začetnico, vse velike črke pretvorimo v male brez da bi izgubili veliko informacije o strukturi in vsebini besedila.\n",
    "\n",
    "3. **Odstranjevanje besed brez pomena**: Če pogledamo porazdelitev besed v besedilu opazimo, da so nekatere besede, ki se pogosto pojavijo brez veliko pomena (v angleščini naprimer \"the\", \"is\", \"and\"). Ker te besede pogosto ne prispevajo k naši analizi, jih pogosto odstranimo.\n",
    "\n",
    "4. **Odstranjevanje ločil**: Pogosto odstranimo tudi ločila, saj ponavadi ne dodajo nič informacije, ki bi bila za nas pomembna.\n",
    "\n",
    "5. **Normalizacija**: Posebaj v slovenščini, se lahko beseda pojavi v zelo različnih oblikah, na primer z različnimi končnicami. Z lematizacijo vse oblike besede poenotimo v eno, imenovano lema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad29862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "135675941b875faa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:34:52.668987Z",
     "start_time": "2023-11-16T09:34:52.659482Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/sebastianmeznar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sebastianmeznar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sebastianmeznar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Naložimo potrebne datoteke, ki nam bodo pomagale pri predprocesiranju\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inicializiramo lematizaro in stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d882c2cab326d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:34:52.702663Z",
     "start_time": "2023-11-16T09:34:52.668595Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sestavimo funkcijo, ki bo besedilo predprocesiralo\n",
    "def preprocess_text(text):\n",
    "    # Besedilo tokeniziramo\n",
    "    words = word_tokenize(text.lower())  # Črke spremenimo v male\n",
    "\n",
    "    # Znebimo se ločil\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(table) for word in words if word.isalpha()]\n",
    "\n",
    "    # Odstranimo pogoste besede z malo informacije\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Besedilo lematiziramo \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Namesto lematizacije lahko s stemmingom odstanimo prefixe in suffixe besed\n",
    "    # stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Predelane besede združimo nazaj v string\n",
    "    preprocessed_text = ' '.join(lemmatized_words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ee3b41714eed72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:36:33.652081Z",
     "start_time": "2023-11-16T09:34:52.677425Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pokličemo funckijo na podatkih\n",
    "data['clean_text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee80b3",
   "metadata": {},
   "source": [
    "Preverimo, kako sedaj besedilo zgleda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c449796b3954a837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:36:33.660283Z",
     "start_time": "2023-11-16T09:36:33.654916Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'relation fortier profiler fact police series violent crime profiler look crispy fortier look classic profiler plot quite simple fortier plot far complicated fortier look like prime suspect spot similarity main character weak weirdo clairvoyance people like compare judge evaluate enjoying funny thing people writing fortier look american hand arguing prefer american series maybe language spirit think series english american way actor really good funny acting superficial'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['clean_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3d3fd1c84778c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:36:33.806122Z",
     "start_time": "2023-11-16T09:36:33.667887Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Podatke razdelimo na učno in testno množico\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['clean_text', 'text']], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2562a4",
   "metadata": {},
   "source": [
    "## Naloga 1: Pretvarjanje besedil v numerične spremenljivke z TD-IDF-jem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd79f47",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency (TD-IDF) je numerična statistična metoda, ki se pri procesiranju naravnega jezika uporablja za ocenjevanje pomembnosti besed v dokumentu znotraj zbirke dokumentov (korpusa).\n",
    "\n",
    "Izračuna se na sledeč način:\n",
    "\n",
    "1. **Term Frequency (TF):** meri kako pogosto se term (beseda) pojavi v besedilu.To izračunamo tako, da število pojav besede delimo s številom besed v dokumentu. Ideja je, da so bolj pogoste besede v besedilu bolj pomembne.\n",
    "\n",
    "   $ \\text{TF}(t, d) = \\frac{\\text{Število pojavitev terma } t \\text{ znotraj dokumenta } d}{\\text{Število vseh pojavitev termov v dokumentu } d} $\n",
    "\n",
    "2. **Inverse Document Frequency (IDF):** Ta del izračuna pomembnost posamezne besede znotraj zbirke dokumentov. Ideja za tem je, da so besede, ki se redko pojavijo znotraj zbirke besedil bolj pomembne.\n",
    "   $ \\text{IDF}(t, D) = \\log{\\left(\\frac{\\text{Število dokumentov v korpusu } D}{\\text{Število dokumentov, ki vsebuje term } t}\\right)} + 1$\n",
    "\n",
    "3. **TF-IDF:** Produkt TF in IDF-ja. Da visoko težo besedam, ki se pogosto pojavijo znotraj sprecifičnega dokumenta, a redko znotraj zbirke dokumentov. Besede, ki se pojavijo v veliko dokumentih imajo torej nizko težo.\n",
    "   $ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $\n",
    "\n",
    "Using TF-IDF, you can represent each document as a numerical vector where each dimension represents a term and its importance in that document. This technique is widely used in information retrieval, text mining, and search engine optimization, helping to determine the relevance of a document to a query or to analyze the significance of terms within documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f4b5a",
   "metadata": {},
   "source": [
    "S pomočjo razreda TfidfVectorizer pretvori predelana besedila v vektorje in preveri točnost Logistične regresije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43cd8439fd644242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T13:08:12.592441Z",
     "start_time": "2023-11-16T13:08:10.627545Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model building: Choose and train a classifier\n",
    "vectorizer = TfidfVectorizer()  # Use TF-IDF vectorizer for text to numerical feature conversion\n",
    "X_train_vec = vectorizer.fit_transform(X_train['clean_text'])\n",
    "X_test_vec = vectorizer.transform(X_test['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e808324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 26948)\t0.12197997694243248\n",
      "  (0, 15564)\t0.08720149867673216\n",
      "  (0, 16152)\t0.15991546982424545\n",
      "  (0, 16823)\t0.0443884874444932\n",
      "  (0, 37959)\t0.0627685189461493\n",
      "  (0, 40833)\t0.07481502408279417\n",
      "  (0, 14558)\t0.10420578054405999\n",
      "  (0, 32434)\t0.0861747712062763\n",
      "  (0, 8788)\t0.17390170767620566\n",
      "  (0, 34792)\t0.1114337595826239\n",
      "  (0, 33044)\t0.04492938464553721\n",
      "  (0, 26674)\t0.09278617082914681\n",
      "  (0, 21515)\t0.17147336398733765\n",
      "  (0, 36543)\t0.12972315448674823\n",
      "  (0, 22475)\t0.09896981699184786\n",
      "  (0, 19348)\t0.0566494986219791\n",
      "  (0, 15748)\t0.16440013915737053\n",
      "  (0, 11981)\t0.16875974843430205\n",
      "  (0, 3188)\t0.275688330118949\n",
      "  (0, 3003)\t0.11231047420134478\n",
      "  (0, 1677)\t0.1368198101436671\n",
      "  (0, 10292)\t0.2751727559467969\n",
      "  (0, 23911)\t0.17252736452640924\n",
      "  (0, 30107)\t0.10555478302023709\n",
      "  (0, 27460)\t0.2669110184294683\n",
      "  (0, 21428)\t0.12619990003080675\n",
      "  (0, 12264)\t0.11098948986499531\n",
      "  (0, 17678)\t0.09845642664564698\n",
      "  (0, 5432)\t0.17635843477964916\n",
      "  (0, 14945)\t0.17781705735745004\n",
      "  (0, 19079)\t0.26050271695301264\n",
      "  (0, 29149)\t0.18594106332462307\n",
      "  (0, 3910)\t0.17982147237024182\n",
      "  (0, 36197)\t0.21315605852484193\n",
      "  (0, 2785)\t0.10047997508510145\n",
      "  (0, 26421)\t0.13816684588301578\n",
      "  (0, 47097)\t0.06977931377551248\n",
      "  (0, 51254)\t0.07283009278457443\n",
      "  (0, 16678)\t0.14932843919436303\n",
      "  (0, 27520)\t0.1528910192776685\n",
      "  (0, 1683)\t0.18197635468717246\n",
      "  (0, 35145)\t0.10672494934697813\n",
      "  (0, 3163)\t0.07076325268148903\n",
      "  (0, 1338)\t0.17418464879402146\n",
      "  (0, 40818)\t0.0924044716553484\n",
      "  (0, 1241)\t0.06798451762751138\n",
      "(16000, 53067)\n",
      "(46,)\n",
      "['aa' 'aaa' 'aaaaaaah' ... 'østbye' 'über' 'üvegtigris']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vec[0])\n",
    "print(X_train_vec.shape)\n",
    "print(X_train_vec[0].nonzero()[1].shape)\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T13:08:34.902466Z",
     "start_time": "2023-11-16T13:08:13.223237Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.88625\n",
      "Random Forest Accuracy: 0.8585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_vec, y_train)\n",
    "logistic_predictions = logistic_model.predict(X_test_vec)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "print(\"Logistic Regression Accuracy:\", logistic_accuracy)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_vec, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_vec)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5a731",
   "metadata": {},
   "source": [
    "## Naloga 2: Pretvarjanje besedil v numberične spremenljivke z vpetji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a95400",
   "metadata": {},
   "source": [
    "Na 8ih vajah smo si ogledali samokodirnike, ki stisnejo originalne podatke v vektorski prostor nizke dimenzije, imenovan latentni prostor. Preslikavi iz originalnega v latentni prostor imenujemo vpetje. Z vpetjem podatkov zmanjšamo razsežnost podatkov in (v primeru dobrega vpetja) ujamemo skrite povezave med podatki. Najbolj znan primer povezav oz lastnosti, ki se pojaviju v vpetju je: od vektorja kralj odštejemo vektor moški in prištejemo vektor ženska in dobimo vektor, ki se dekodira v besedo kraljica.\n",
    "\n",
    "En izmed najpopularnejših pristopov za vpetje besed v vektorski prostor je Word2Vec. Ideja za pristopom je, da se besede s podobnim pomenom pojavijo v podobnih kontekstih in morajo zato biti njihove predstavitve v latentnem prostoru blizu.\n",
    "\n",
    "Obstajata dve glavni arhitekturi za Word2vec:\n",
    "\n",
    "1. **Continuous Bag-of-Words (CBOW):** Model napoveduje verjetnost ciljne besede glede na dane besedi v njeni okolici. Če na primer podamo besede \"mačka sedi na\", bo model napovedal \"preprogi\".\n",
    "\n",
    "2. **Skip-gram:** model deluje v obratni smeri. Na vhod dobi besedo \"preproga\" in poskusi napovedati besede v okolici, torej \"mačka\", \"sedi\", \"na\"\n",
    "\n",
    "Oba modela uporabljata usmerjeno nevronsko mrežo z enim samim skritim slojem, s katero se učimo uteži za predstavitev besed z vektorjem. Uteži skritega sloja postanejo vpetja oziroma predstavitev besed z vektorjem.\n",
    "\n",
    "Word2Vec je imel na procesiranje naravnega jezika velik vpliv, saj se njegova vpetja lahko uporabi za različne naloge, ki so povezane z besedili. En izmed razlogov za to je, da zelo dobro ujame semantične povezave med besedami."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0831b8e8",
   "metadata": {},
   "source": [
    "2.a: Natreniraj model Word2Vec iz knjižnjice gensim ([gensim.models.Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)). Uporabi parametre: vector_size=100, window=5, min_count=1, workers=4, epochs=10. Pred trening vsako besedilo v učni in testni množici razreži na besede s funkcijo `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "525e30d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from gensim) (1.12.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: wrapt in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Downloading gensim-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.2 smart-open-7.0.4\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9828965dfd3a0446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:37:06.410472Z",
     "start_time": "2023-11-16T09:36:57.766561Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_train_text = [text.split() for text in X_train['clean_text']]\n",
    "tokenized_test_text = [text.split() for text in X_test['clean_text']]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(tokenized_train_text, vector_size=100, window=5, min_count=1, workers=4, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2906a",
   "metadata": {},
   "source": [
    "2.b: Model Word2Vec si shrani vse besede in pripadajoče vektorje v spremenljivki wv. Vse besede v vokabularju lahko dobimo v spremenljivki `model.wv.index_to_key`, vektorje pa z ukazom `model.wv[beseda]`. Sestavi slover iz prvih petih besed in pripadajočih vektorjev ter gesla in vrednosti v slovarju izpiši."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "725eba6b29d0d617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:37:06.423932Z",
     "start_time": "2023-11-16T09:37:06.409648Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: br\n",
      "Vector: [ 1.0304588  -0.99352753  1.2502333   0.9991075  -0.37750974 -0.95246685\n",
      " -0.08215574  1.0067925  -0.45983577  0.72306514  0.04959567 -0.9132974\n",
      " -0.8457528   1.0041964  -0.9317955  -1.4430221  -0.28187922  0.43200836\n",
      " -0.6437984   0.44460648  0.02092529 -0.8753164   1.9030688   1.523923\n",
      "  1.1657525  -0.16998577  0.83281416 -0.11189803 -0.8396603  -0.13254046\n",
      "  0.28827408  1.3844548   0.8737769  -0.33922812 -1.3191979  -0.8705202\n",
      " -0.72890854 -0.7306533  -0.64392984 -1.870423    0.11011513  1.8222171\n",
      " -0.08509613 -0.05540688 -1.3797641  -1.2390989  -2.1060123  -0.5156891\n",
      "  0.63929766  0.5577973  -0.14040232  0.20853749  1.2979497  -0.5068703\n",
      "  0.7383669  -0.13384515  0.40067765 -0.8670849  -0.6145245   1.0474917\n",
      " -0.5265668  -1.2710965  -1.8136395  -2.9256155   0.8904954  -0.3156277\n",
      " -0.4164132  -0.91932905 -1.3663757   0.64272034  1.522156    0.68175864\n",
      " -0.07323485 -0.2420179  -0.678943    0.7326429   0.625479   -0.840906\n",
      "  0.49216625 -0.726544   -0.08493825 -0.45517087 -0.20743434  2.4038782\n",
      " -1.6317536   0.8910659  -1.171382    1.2055489  -0.79073554 -0.8580467\n",
      "  0.24588205 -0.87328124  0.12112296  1.7925895  -0.10670692 -0.21021676\n",
      " -1.1875018  -0.3644941  -1.0503315  -0.29649687]\n",
      "\n",
      "\n",
      "Word: movie\n",
      "Vector: [-1.0063534  -0.8819411   1.643566    1.955582   -0.02848363 -0.84215504\n",
      " -1.0512837   1.1984429  -0.32258672 -0.49545813 -0.8909537  -0.6018178\n",
      "  0.24429017  1.098467    0.36959666 -0.09245408  2.2179985   1.8535167\n",
      " -1.2443416  -0.9823427   0.9991899   0.04062577  1.8214872   0.7897759\n",
      "  1.5656706  -0.3250942   0.05302716  0.6963285  -1.6339041  -0.3344996\n",
      "  1.1884733  -1.2482053   0.9388415  -2.4872415  -1.6200078  -1.8238418\n",
      "  0.45259127 -0.9515112   0.11383455  0.93822116  0.3396994   2.2525976\n",
      "  0.01862758 -0.78045887  0.14751996  0.65092266  0.68857497  0.11367165\n",
      "  0.5757105   1.1838146   0.10005344  0.1065653   0.76781493 -0.52732015\n",
      "  0.30881754 -1.8633186   0.64466816 -0.7718275  -0.953698    1.4467031\n",
      " -0.7709414  -0.6685058   0.21551499 -3.4899228  -1.860454    0.15122136\n",
      "  1.4230103   0.78399026 -0.5203448   1.3346316   0.7216675  -0.3664841\n",
      " -1.8734584  -0.07187199  1.1219653  -0.96073604  1.6452432  -0.32487848\n",
      "  0.11823137 -0.61670345 -1.2327279   0.3992268   0.65757394  1.0482923\n",
      " -1.8537194   0.73056895 -1.1927863  -1.9775596  -0.604647    0.02245737\n",
      " -0.14111236 -0.9218342   0.5343244  -0.8068586   0.26861843  1.6371096\n",
      " -1.1080922   0.32455108 -0.39223874  0.71844375]\n",
      "\n",
      "\n",
      "Word: film\n",
      "Vector: [-1.0769109  -1.230283    1.4194467   2.599116   -0.4649121  -1.2527306\n",
      " -0.8553912   0.98725384 -0.39026186  0.13928343 -0.8643474  -0.5070024\n",
      "  0.9286117   0.84309655  0.25076553  0.30486304  0.8151802   2.907131\n",
      " -1.2227706  -0.7999838  -0.03709255 -0.40754366  1.920784    0.11195157\n",
      "  2.4159255  -1.3883351   0.21085416  0.58957916 -1.736886    0.16781826\n",
      "  0.8125774  -1.2958952   0.74250793 -1.429381   -2.393904   -1.8390843\n",
      "  1.0099918  -0.37529442 -0.10734151  0.22107106  0.00549651  1.2370859\n",
      " -0.6785128  -0.99425924  1.4192196   1.1302049  -0.6416723  -0.4300584\n",
      "  1.7693694  -0.10649258 -0.05392188  0.23314017 -0.54425156 -0.55152035\n",
      "  1.460073    0.26318285  2.2388146  -1.8101014  -1.6065266   0.7118023\n",
      " -0.04973687 -0.3917772   0.40144223 -2.7882419  -0.72837895  0.03041361\n",
      " -0.56498545  0.878472    0.19984226  1.2357059   0.73678094 -0.37682077\n",
      " -0.310561   -0.35488236  1.1795065  -0.2399542   2.4114983  -0.3699301\n",
      "  0.389409   -0.7168578  -1.4055474   0.15605898  0.42439577  0.18795621\n",
      " -1.1933638  -0.4283658  -0.89565974 -1.1695608   0.11662177 -0.8213165\n",
      " -0.42830682 -1.3095672   0.5329901  -0.64329535  0.31959638  1.9685136\n",
      " -1.0055839   0.76019394  0.67016584  0.1454543 ]\n",
      "\n",
      "\n",
      "Word: one\n",
      "Vector: [-2.3887043e+00 -1.2803510e+00 -2.1786761e-01  2.0542440e+00\n",
      "  1.1438497e+00 -9.2995548e-01  7.1294218e-01  6.4415336e-01\n",
      "  1.0267141e-01 -1.5762668e+00  2.0357816e+00  5.6903273e-01\n",
      " -8.2125738e-02  2.1004911e-01 -4.5125338e-01 -8.9901400e-01\n",
      "  1.3101832e+00 -8.1379551e-01 -1.4247093e+00 -5.1994443e-01\n",
      "  4.8076952e-01 -7.8300184e-01  2.7296693e+00  1.9478571e+00\n",
      " -7.0037323e-01 -8.6896777e-02 -2.3729375e-01 -4.3773562e-01\n",
      " -1.4725397e+00  5.6064904e-01  1.4712011e+00 -2.0298383e+00\n",
      " -4.1265383e-01 -4.9188086e-01  1.8497944e-01 -2.9798925e-01\n",
      "  1.0627879e+00 -1.6329597e+00 -1.0271773e+00  7.7386600e-01\n",
      "  3.3375835e-01  1.4361854e+00  5.9000731e-01 -1.2943093e+00\n",
      "  1.0194974e+00 -3.7177181e-01  1.2855492e+00  2.6086077e-01\n",
      "  8.9892000e-01  1.4578539e+00  4.5711735e-01  2.1866854e-01\n",
      "  1.3093124e-01 -7.8648621e-01  4.5867628e-01 -3.6754575e-01\n",
      "  1.8987586e-01  2.2578830e-01  8.1581622e-02  1.4915638e-01\n",
      " -2.0020606e-02  5.3627777e-01  5.4735191e-02 -2.2355127e+00\n",
      "  7.9279560e-01 -1.1134777e+00 -1.1231629e+00  1.7909362e+00\n",
      " -5.2504575e-01  8.3053589e-02  1.7310439e-01 -6.2519956e-01\n",
      "  3.9917248e-01 -4.7378063e-01 -1.5636988e-01 -4.7180882e-01\n",
      "  1.6849411e+00  7.4211842e-01 -1.3044195e-01  2.6704505e-01\n",
      " -1.7454035e+00  1.0523634e+00 -5.1647466e-01  1.3348883e+00\n",
      " -4.4108069e-01  1.7064539e+00 -2.9523033e-01  5.5782124e-02\n",
      " -3.2279462e-02 -4.0316704e-01 -1.6432991e+00 -9.7367263e-01\n",
      " -4.4032091e-01  2.8035879e-01 -7.7841705e-01 -1.5516045e-03\n",
      " -9.5638835e-01 -1.1367511e+00 -7.9140741e-01 -1.4648333e-01]\n",
      "\n",
      "\n",
      "Word: like\n",
      "Vector: [-1.79397893e+00  3.85020375e-02 -1.72753477e+00  3.43489796e-01\n",
      " -8.80057644e-03 -2.38131976e+00  3.26593369e-01  6.54229820e-01\n",
      "  1.06549013e+00 -2.04907614e-03 -2.10762754e-01 -3.52726728e-01\n",
      " -1.15422976e+00  1.42063391e+00 -7.14595020e-01 -4.74784911e-01\n",
      " -6.09435022e-01  1.03261292e+00  1.41336977e+00 -4.51925367e-01\n",
      "  5.97752810e-01 -8.97850633e-01  1.15358472e+00  2.47415304e+00\n",
      " -1.16354814e-02  3.92957091e-01 -3.48457433e-02 -2.56784171e-01\n",
      " -2.00353098e+00  2.35540166e-01  2.32341433e+00  5.01198411e-01\n",
      "  2.26199538e-01 -7.07853913e-01  5.05180776e-01 -3.68199658e+00\n",
      " -1.44462073e+00 -6.38166726e-01 -8.28423262e-01 -6.10467196e-01\n",
      " -4.50894922e-01 -3.84192288e-01 -8.64958093e-02  2.38342977e+00\n",
      "  1.01352654e-01 -1.04270005e+00  3.36286485e-01  3.04517418e-01\n",
      " -3.06462288e-01 -5.64751215e-02  6.20578900e-02  8.29195380e-01\n",
      "  1.50417161e+00 -5.14523327e-01 -5.55514634e-01 -2.43345642e+00\n",
      " -5.18489063e-01 -7.61488616e-01  6.39233142e-02 -1.79038453e+00\n",
      " -2.38597035e-01 -1.93559945e+00  1.76360622e-01  6.60983384e-01\n",
      " -8.96732211e-01 -1.21881497e+00  1.51256073e+00  1.23352206e+00\n",
      " -1.98300254e+00  3.47407699e+00  6.89951926e-02  8.75768363e-01\n",
      " -7.99582183e-01  4.32285577e-01  1.13261200e-01 -1.54422033e+00\n",
      "  1.97205231e-01  1.26724565e+00  1.53307021e-01 -4.94787514e-01\n",
      " -2.18509841e+00 -3.19886267e-01 -3.56981248e-01 -2.88913846e-01\n",
      " -2.95857859e+00  5.10069847e-01  1.10335445e+00  2.00899914e-01\n",
      " -1.91171265e+00  7.15818942e-01 -5.11602581e-01 -9.39201593e-01\n",
      "  1.00210798e+00  3.06382217e-02  1.96342766e-01 -2.56310320e+00\n",
      "  8.34351778e-01  2.39646626e+00 -6.76819026e-01  2.48389557e-01]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all words and their vectors in the Word2Vec model's vocabulary\n",
    "all_words = w2v_model.wv.index_to_key\n",
    "word_vectors = {word: w2v_model.wv[word] for word in all_words[:5]}\n",
    "\n",
    "# Print the word vectors\n",
    "for word, vector in word_vectors.items():\n",
    "    print(f\"Word: {word}\")\n",
    "    print(f\"Vector: {vector}\")\n",
    "    print(\"\\n\")  # Add a newline for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28edebb",
   "metadata": {},
   "source": [
    "2.c: Ukazom `model.wv.most_similar` in parametrom topn, lahko najdemo n najbolj podobnih besed znotraj slovarja. Izpiši prvih 10 najbližjih besed besedi \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb26811258d5611c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:06:08.276810Z",
     "start_time": "2023-11-15T17:04:40.472606Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar word: dog, Similarity: 0.7748943567276001\n",
      "Similar word: mouse, Similarity: 0.7126650810241699\n",
      "Similar word: bird, Similarity: 0.6762747764587402\n",
      "Similar word: soup, Similarity: 0.665956437587738\n",
      "Similar word: flavoured, Similarity: 0.6533246040344238\n",
      "Similar word: fish, Similarity: 0.6445339918136597\n",
      "Similar word: satan, Similarity: 0.6432642340660095\n",
      "Similar word: kitty, Similarity: 0.6415029168128967\n",
      "Similar word: wolf, Similarity: 0.6397603750228882\n",
      "Similar word: hat, Similarity: 0.6393377780914307\n"
     ]
    }
   ],
   "source": [
    "# Find similar words to a specific word\n",
    "similar_words = w2v_model.wv.most_similar('cat', topn=10)\n",
    "\n",
    "# 'word' is the word for which you want to find similar words, and 'topn' specifies the number of similar words to retrieve\n",
    "\n",
    "# Print the similar words and their similarity scores\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"Similar word: {word}, Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87cb8c5",
   "metadata": {},
   "source": [
    "2.d: Model Word2Vec vsaki besedi priredi vektor. V naši nalogi delamo z besedili, ki so dolga več besed zato moramo te vektorje nekako zagregirat, na primer tako, da vektorje besed znotraj vsakega besedila povprečimo. Definiraj funkcijo, ki bo vsako besedilo iz učne in testne množice spremenila v vektor dolžine 100 (če v besedilu ni nobene besede naj bo to vektor ničel). Z dobljeno množico vektorjev preveri točnost logistične regresije za klasifikacijo iz naloge 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca1b774973aa6131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:06:08.277006Z",
     "start_time": "2023-11-15T17:04:40.539008Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to get average Word2Vec representation for a sentence\n",
    "def get_average_w2v(tokens):\n",
    "    vector_sum = 0\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        if word in w2v_model.wv:\n",
    "            vector_sum += w2v_model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        return vector_sum / count\n",
    "    else:\n",
    "        return [0] * 100  # Return zero vector if no word found\n",
    "\n",
    "# Add Word2Vec representations to DataFrame\n",
    "X_train_w2v = [get_average_w2v(text) for text in tokenized_train_text]\n",
    "X_test_w2v = [get_average_w2v(text) for text in tokenized_test_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ee0492233050a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:06:08.277448Z",
     "start_time": "2023-11-15T17:04:45.600037Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8545\n",
      "Random Forest Accuracy: 0.83025\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train_w2v, y_train)\n",
    "logistic_predictions = logistic_model.predict(X_test_w2v)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_predictions)\n",
    "print(\"Logistic Regression Accuracy:\", logistic_accuracy)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train_w2v, y_train)\n",
    "rf_predictions = rf_model.predict(X_test_w2v)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d0044e6d28390",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Naloga 3: Vnaprej naučeni modeli\n",
    "\n",
    "Zadnje čase se za procesiranje naravnega jezika uporabljajo predvsem vnaprej naučeni modeli. Vnaprej naučeni modeli so (ponavadi) velike nevronske mreže, ki so naučeni na veliki množici podatkov. Vnaprej naučene modele ponavadi uporabimo kot začetno točko, ki jo dotreniramo za našo nalogo. Ti modeli so uporabni iz več razlogov.\n",
    "\n",
    "1. **Generalizacija**: Vnaprej naučeni modeli so naučeni na velikih in raznolikih besedilnih korpusih, kar jim omogoča učenje posplošenih reprezentacij jezika. To jim omogoča, da se dokaj dobro izkažejo pri številnih nadaljnjih nalogah brez potrebe dotreniranja za posamezno nalogo.\n",
    "\n",
    "2. **Učinkovitost virov**: Uporaba Vnaprej naučenih modelov prihrani računalniške vire in čas. Namesto da bi uporabnik modele treniral od začetka, za kar potrebuje precej podatkov in računalniške moči, lahko izkoristi te vnaprej obstoječe, dobro trenirane modele.\n",
    "\n",
    "3. **Transfer learning**: Vnaprej naučeni modeli omogočajo, da znanje, pridobljeno pri eni nalogi, prenesemo na drugo sorodno nalogo. S dotreniranjem (finetuningom) na določenih naborih podatkov ali nalogah se lahko njihova zmogljivost znatno izboljša z minimalnim dodatnim učenjem.\n",
    "\n",
    "Dotreniranje se nanaša na postopek, pri katerem se vzame vnaprej naučen model in ga dotrenira na posebnem naboru podatkov ali nalogi. Posledično se njegovi parametri prilagodijo za boljše delovanje v tem posebnem kontekstu. Dotreniranje je pomembno saj:\n",
    "\n",
    "- **Prilagajanje na posamezno nalogo**: Dotreniranje omogoča modelu, da se prilagodi podatkov ali nalogi.\n",
    "\n",
    "- **Povečana zmogljivost**: Z dotreniranjem na podatkih, specifičnih za domeno ali nalogo, se lahko model nauči več značilnosti, specifičnih za nalogo, kar izboljša natančnost in učinkovitost za predvideno uporabo.\n",
    "\n",
    "- **Zmanjšana zahteva po podatkih**: Za dotreniranje modela je pogosto potrebnih manj podatkov kot za učenje modela od začetka. Če začnemo z vnaprej naučenim modelom, se lahko učinkovito učimo iz manjšega nabora podatkov, specifičnega za določeno področje, kar je koristno v scenarijih, kjer je na voljo omejena količina anotiranih podatkov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfc7d531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from transformers) (0.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe122bc5ca65fc3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:06:08.277692Z",
     "start_time": "2023-11-15T17:04:59.863546Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE, Confidence: 0.9999\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Inicializiramo model za analizo sentimenta\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "text = \"I absolutely love this product! It's fantastic!\"\n",
    "\n",
    "# Naredimo analizo sentimenta za zgornji vzorec\n",
    "result = sentiment_analysis(text)\n",
    "\n",
    "# Izpišemo sentiment vzorca in koliko je model \"prepričan\" v napoved\n",
    "print(f\"Sentiment: {result[0]['label']}, Confidence: {result[0]['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c99d17",
   "metadata": {},
   "source": [
    "3.a: Preveri, če se sentiment prvih 500 primerov sklada s ciljnimi vrednosti (torej, če model vrne \"POSITIVE\" je napovedana vrednost \"pos\", če ne \"neg\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb84f74e7a503539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:12:32.375177Z",
     "start_time": "2023-11-15T17:10:15.076512Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:22<00:00, 22.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Accuracy: 0.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "trans_y_pred = []\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "n = 500\n",
    "\n",
    "for test_text in tqdm(X_test['text'][:n]):\n",
    "    result = sentiment_analysis(test_text[:1500])\n",
    "    sentiment = result[0]['label']\n",
    "    trans_y_pred.append('pos' if sentiment == 'POSITIVE' else 'neg')\n",
    "\n",
    "trans_accuracy = accuracy_score(y_test_reset[:n], trans_y_pred)\n",
    "print(\"Transformer Accuracy:\", trans_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7cbb0",
   "metadata": {},
   "source": [
    "## Dodatna naloga (v ang.): Stanza & POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a4bc0d8cd9989",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Stanza is an NLP library developed by the Stanford NLP Group. It's designed for a wide range of natural language processing tasks, including tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more. Stanza aims to provide efficient and accurate pre-trained models for various languages.\n",
    "\n",
    "Key features of Stanza include:\n",
    "- **Pre-Trained Models**: Stanza comes with pre-trained models for multiple languages, allowing users to perform various NLP tasks without training models from scratch.\n",
    "- **Ease of Use**: It offers a simple and intuitive API for performing different NLP tasks, making it accessible for both beginners and experienced researchers.\n",
    "- **Accuracy**: Stanza models are known for their high accuracy in different NLP tasks due to their robust training on extensive datasets.\n",
    "- **Multiple Languages**: Stanza supports multiple languages, making it suitable for multilingual NLP applications.\n",
    "\n",
    "Stanza provides state-of-the-art performance in various NLP tasks and continues to evolve with advancements in the field of natural language processing.\n",
    "\n",
    "### Use Case: Text Analysis with Universal POS Tagging using Stanza\n",
    "\n",
    "Stanza's Universal POS tagging can be highly beneficial in various text analysis tasks. Let's consider a scenario where you have a dataset of customer reviews for a product. By utilizing Stanza's Universal POS tagging, you can perform the following analysis:\n",
    "\n",
    "1. **Extracting Key Features**: Identify the key features or attributes of the product mentioned in the reviews by analyzing nouns (NOUN) and adjectives (ADJ) tagged using Stanza. This helps in understanding what aspects of the product are being praised or criticized.\n",
    "\n",
    "2. **Sentiment Analysis**: Analyze sentiments associated with specific parts of speech. For instance, adjectives (ADJ) often reflect sentiments or opinions. By associating adjectives with their corresponding nouns, you can determine the sentiment expressed towards various product features.\n",
    "\n",
    "3. **Customer Feedback Categorization**: Categorize customer feedback into different categories based on the identified parts of speech. For instance, categorize reviews mentioning \"customer service\" (PROPN) separately to analyze the sentiment specifically related to that aspect.\n",
    "\n",
    "4. **Comparative Analysis**: Compare the frequency and sentiment of different parts of speech across different products or time frames to identify trends and patterns in customer opinions.\n",
    "\n",
    "By utilizing Stanza's Universal POS tagging, you can effectively extract meaningful insights from textual data, enabling better decision-making and improving products or services based on customer feedback.\n",
    "\n",
    "### Universal POS Tags\n",
    "- **ADJ**: Adjective\n",
    "- **ADP**: Adposition\n",
    "- **ADV**: Adverb\n",
    "- **AUX**: Auxiliary\n",
    "- **CCONJ**: Coordinating conjunction\n",
    "- **DET**: Determiner\n",
    "- **INTJ**: Interjection\n",
    "- **NOUN**: Noun\n",
    "- **NUM**: Numeral\n",
    "- **PART**: Particle\n",
    "- **PRON**: Pronoun\n",
    "- **PROPN**: Proper noun\n",
    "- **PUNCT**: Punctuation\n",
    "- **SCONJ**: Subordinating conjunction\n",
    "- **SYM**: Symbol\n",
    "- **VERB**: Verb\n",
    "- **X**: Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a0bab1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.12.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from stanza) (1.26.4)\n",
      "Collecting protobuf>=3.15.0 (from stanza)\n",
      "  Downloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: networkx in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from stanza) (3.3)\n",
      "Collecting toml (from stanza)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from stanza) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from stanza) (4.66.2)\n",
      "Requirement already satisfied: filelock in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza) (12.4.127)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from requests->stanza) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from requests->stanza) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from requests->stanza) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Downloading stanza-1.8.2-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.1/990.1 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.26.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: toml, protobuf, emoji, stanza\n",
      "Successfully installed emoji-2.12.1 protobuf-5.26.1 stanza-1.8.2 toml-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d729e48866f6451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:04:16.150895Z",
     "start_time": "2023-11-16T10:04:07.452456Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 38.0MB/s]                    \n",
      "2024-05-23 14:15:01 INFO: Downloaded file to /home/sebastianmeznar/stanza_resources/resources.json\n",
      "2024-05-23 14:15:01 INFO: Downloading default packages for language: en (English) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.8.0/models/default.zip: 100%|██████████| 527M/527M [00:09<00:00, 55.0MB/s] \n",
      "2024-05-23 14:15:12 INFO: Downloaded file to /home/sebastianmeznar/stanza_resources/en/default.zip\n",
      "2024-05-23 14:15:14 INFO: Finished downloading models and saved to /home/sebastianmeznar/stanza_resources\n",
      "2024-05-23 14:15:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.8.0.json: 379kB [00:00, 43.4MB/s]                    \n",
      "2024-05-23 14:15:14 INFO: Downloaded file to /home/sebastianmeznar/stanza_resources/resources.json\n",
      "2024-05-23 14:15:14 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-05-23 14:15:15 INFO: Loading these models for language: en (English):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "2024-05-23 14:15:15 INFO: Using device: cuda\n",
      "2024-05-23 14:15:15 INFO: Loading: tokenize\n",
      "2024-05-23 14:15:15 INFO: Loading: mwt\n",
      "2024-05-23 14:15:15 INFO: Loading: pos\n",
      "2024-05-23 14:15:15 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Nouns: ['camera', 'quality', 'battery', 'life']\n",
      "Extracted Adjectives: ['amazing', 'better']\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download English model (change 'en' to the appropriate language code if needed)\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize the English pipeline\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos')\n",
    "\n",
    "# Sample customer review\n",
    "sample_review = \"The camera quality is amazing, but the battery life could be better.\"\n",
    "\n",
    "# Process the review\n",
    "doc = nlp(sample_review)\n",
    "\n",
    "# Extract nouns and adjectives\n",
    "nouns = []\n",
    "adjectives = []\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.upos == 'NOUN':\n",
    "            nouns.append(word.text)\n",
    "        elif word.upos == 'ADJ':\n",
    "            adjectives.append(word.text)\n",
    "\n",
    "# Print extracted nouns and adjectives\n",
    "print(\"Extracted Nouns:\", nouns)\n",
    "print(\"Extracted Adjectives:\", adjectives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af1b55",
   "metadata": {},
   "source": [
    "Prirejeno pa vajah Boshko-ta Koloskega (Inteligentni sistemi, FRI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
