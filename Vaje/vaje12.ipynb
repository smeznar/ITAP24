{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ded8fd",
   "metadata": {},
   "source": [
    "# Vaje 12: Analiza besedil in vpetja podatkov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b76e602f9222128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:34:47.041946Z",
     "start_time": "2023-11-16T09:34:46.967739Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0acaed9",
   "metadata": {},
   "source": [
    "Uporabljali bomo podatkovno množico recenzij filmov iz IMDB-ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5a728c063f9569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:34:52.668218Z",
     "start_time": "2023-11-16T09:34:46.992678Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There is no relation at all between Fortier an...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is a great. The plot is very true t...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>George P. Cosmatos' \"Rambo: First Blood Part I...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In the process of trying to establish the audi...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yeh, I know -- you're quivering with excitemen...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0  There is no relation at all between Fortier an...   pos\n",
       "1  This movie is a great. The plot is very true t...   pos\n",
       "2  George P. Cosmatos' \"Rambo: First Blood Part I...   neg\n",
       "3  In the process of trying to establish the audi...   pos\n",
       "4  Yeh, I know -- you're quivering with excitemen...   neg"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Naloži podatkovno množico IMDb\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "\n",
    "# Izbremo 20000 podatkov iz učne množice\n",
    "data = pd.DataFrame(imdb_dataset['train'].shuffle(seed=42).select(range(20000)))\n",
    "\n",
    "# Ciljno vrednost spremenimo iz True v \"pos\" in iz False v \"neg\"\n",
    "data['label'] = data['label'].apply(lambda x: 'pos' if x else 'neg')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3159d682b2038a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T16:33:30.337646Z",
     "start_time": "2023-11-15T16:33:29.952699Z"
    },
    "collapsed": false
   },
   "source": [
    "# Predprocesiranje: Čiščenje podatkov\n",
    "\n",
    "Ena izmed najbolj pomembnih nalog pri analizi besedil je predprocesiranje podatkov. Besedila moramo torej spraviti v format, ki je ustrezen za algoritme strojnega učenja (torej matrike/tenzorje). Poglejmo si torej nekaj tehnik predprocesiranja, ki so skoraj obvezne, ko delamo z besedili:\n",
    "\n",
    "1. **Tokenizacija**: Čeprav na besede ponavadi gledamo kot na celoto, so le-te ponavadi sestavljene iz več delov, ki so skupni več besedam (naprimer: nogomet, rokomet, ...). Da bolje ujamemo te povezave med besedami, jih zato ponavadi razbijemo na manjše dele. Dodatno nam tokenizacija omogoča sestavljanje/kodiranje novih (še ne videnih) besed in manjšo množico vhodnih besed.\n",
    "\n",
    "2. **Lowercasing**: Da se izognemu razlikovanja med isto besedo, ko je napisana z veliko in malo začetnico, vse velike črke pretvorimo v male brez da bi izgubili veliko informacije o strukturi in vsebini besedila.\n",
    "\n",
    "3. **Odstranjevanje besed brez pomena**: Če pogledamo porazdelitev besed v besedilu opazimo, da so nekatere besede, ki se pogosto pojavijo brez veliko pomena (v angleščini naprimer \"the\", \"is\", \"and\"). Ker te besede pogosto ne prispevajo k naši analizi, jih pogosto odstranimo.\n",
    "\n",
    "4. **Odstranjevanje ločil**: Pogosto odstranimo tudi ločila, saj ponavadi ne dodajo nič informacije, ki bi bila za nas pomembna.\n",
    "\n",
    "5. **Normalizacija**: Posebaj v slovenščini, se lahko beseda pojavi v zelo različnih oblikah, na primer z različnimi končnicami. Z lematizacijo vse oblike besede poenotimo v eno, imenovano lema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad29862",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135675941b875faa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:34:52.668987Z",
     "start_time": "2023-11-16T09:34:52.659482Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Naložimo potrebne datoteke, ki nam bodo pomagale pri predprocesiranju\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Inicializiramo lematizaro in stemmer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d882c2cab326d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:34:52.702663Z",
     "start_time": "2023-11-16T09:34:52.668595Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sestavimo funkcijo, ki bo besedilo predprocesiralo\n",
    "def preprocess_text(text):\n",
    "    # Besedilo tokeniziramo\n",
    "    words = word_tokenize(text.lower())  # Črke spremenimo v male\n",
    "\n",
    "    # Znebimo se ločil\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    words = [word.translate(table) for word in words if word.isalpha()]\n",
    "\n",
    "    # Odstranimo pogoste besede z malo informacije\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Besedilo lematiziramo \n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Namesto lematizacije lahko s stemmingom odstanimo prefixe in suffixe besed\n",
    "    # stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Predelane besede združimo nazaj v string\n",
    "    preprocessed_text = ' '.join(lemmatized_words)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31ee3b41714eed72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:36:33.652081Z",
     "start_time": "2023-11-16T09:34:52.677425Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pokličemo funckijo na podatkih\n",
    "data['clean_text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ee80b3",
   "metadata": {},
   "source": [
    "Preverimo, kako sedaj besedilo zgleda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449796b3954a837",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:36:33.660283Z",
     "start_time": "2023-11-16T09:36:33.654916Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['clean_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3d3fd1c84778c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:36:33.806122Z",
     "start_time": "2023-11-16T09:36:33.667887Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Podatke razdelimo na učno in testno množico\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['clean_text', 'text']], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2562a4",
   "metadata": {},
   "source": [
    "## Naloga 1: Pretvarjanje besedil v numerične spremenljivke z TD-IDF-jem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd79f47",
   "metadata": {},
   "source": [
    "Term Frequency-Inverse Document Frequency (TD-IDF) je numerična statistična metoda, ki se pri procesiranju naravnega jezika uporablja za ocenjevanje pomembnosti besed v dokumentu znotraj zbirke dokumentov (korpusa).\n",
    "\n",
    "Izračuna se na sledeč način:\n",
    "\n",
    "1. **Term Frequency (TF):** meri kako pogosto se term (beseda) pojavi v besedilu.To izračunamo tako, da število pojav besede delimo s številom besed v dokumentu. Ideja je, da so bolj pogoste besede v besedilu bolj pomembne.\n",
    "\n",
    "   $ \\text{TF}(t, d) = \\frac{\\text{Število pojavitev terma } t \\text{ znotraj dokumenta } d}{\\text{Število vseh pojavitev termov v dokumentu } d} $\n",
    "\n",
    "2. **Inverse Document Frequency (IDF):** Ta del izračuna pomembnost posamezne besede znotraj zbirke dokumentov. Ideja za tem je, da so besede, ki se redko pojavijo znotraj zbirke besedil bolj pomembne.\n",
    "   $ \\text{IDF}(t, D) = \\log{\\left(\\frac{\\text{Število dokumentov v korpusu } D}{\\text{Število dokumentov, ki vsebuje term } t}\\right)} + 1$\n",
    "\n",
    "3. **TF-IDF:** Produkt TF in IDF-ja. Da visoko težo besedam, ki se pogosto pojavijo znotraj sprecifičnega dokumenta, a redko znotraj zbirke dokumentov. Besede, ki se pojavijo v veliko dokumentih imajo torej nizko težo.\n",
    "   $ \\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D) $\n",
    "\n",
    "Using TF-IDF, you can represent each document as a numerical vector where each dimension represents a term and its importance in that document. This technique is widely used in information retrieval, text mining, and search engine optimization, helping to determine the relevance of a document to a query or to analyze the significance of terms within documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794f4b5a",
   "metadata": {},
   "source": [
    "1.a: S pomočjo razreda TfidfVectorizer pretvori predelana besedila v vektorje in preveri točnost Logistične regresije."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43cd8439fd644242",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T13:08:12.592441Z",
     "start_time": "2023-11-16T13:08:10.627545Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fc5a731",
   "metadata": {},
   "source": [
    "## Naloga 2: Pretvarjanje besedil v numberične spremenljivke z vpetji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a95400",
   "metadata": {},
   "source": [
    "Na 8ih vajah smo si ogledali samokodirnike, ki stisnejo originalne podatke v vektorski prostor nizke dimenzije, imenovan latentni prostor. Preslikavi iz originalnega v latentni prostor imenujemo vpetje. Z vpetjem podatkov zmanjšamo razsežnost podatkov in (v primeru dobrega vpetja) ujamemo skrite povezave med podatki. Najbolj znan primer povezav oz lastnosti, ki se pojaviju v vpetju je: od vektorja kralj odštejemo vektor moški in prištejemo vektor ženska in dobimo vektor, ki se dekodira v besedo kraljica.\n",
    "\n",
    "En izmed najpopularnejših pristopov za vpetje besed v vektorski prostor je Word2Vec. Ideja za pristopom je, da se besede s podobnim pomenom pojavijo v podobnih kontekstih in morajo zato biti njihove predstavitve v latentnem prostoru blizu.\n",
    "\n",
    "Obstajata dve glavni arhitekturi za Word2vec:\n",
    "\n",
    "1. **Continuous Bag-of-Words (CBOW):** Model napoveduje verjetnost ciljne besede glede na dane besedi v njeni okolici. Če na primer podamo besede \"mačka sedi na\", bo model napovedal \"preprogi\".\n",
    "\n",
    "2. **Skip-gram:** model deluje v obratni smeri. Na vhod dobi besedo \"preproga\" in poskusi napovedati besede v okolici, torej \"mačka\", \"sedi\", \"na\"\n",
    "\n",
    "Oba modela uporabljata usmerjeno nevronsko mrežo z enim samim skritim slojem, s katero se učimo uteži za predstavitev besed z vektorjem. Uteži skritega sloja postanejo vpetja oziroma predstavitev besed z vektorjem.\n",
    "\n",
    "Word2Vec je imel na procesiranje naravnega jezika velik vpliv, saj se njegova vpetja lahko uporabi za različne naloge, ki so povezane z besedili. En izmed razlogov za to je, da zelo dobro ujame semantične povezave med besedami."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0831b8e8",
   "metadata": {},
   "source": [
    "2.a: Natreniraj model Word2Vec iz knjižnjice gensim ([gensim.models.Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)). Uporabi parametre: vector_size=100, window=5, min_count=1, workers=4, epochs=10. Pred trening vsako besedilo v učni in testni množici razreži na besede s funkcijo `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9828965dfd3a0446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:37:06.410472Z",
     "start_time": "2023-11-16T09:36:57.766561Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24b2906a",
   "metadata": {},
   "source": [
    "2.b: Model Word2Vec si shrani vse besede in pripadajoče vektorje v spremenljivki wv. Vse besede v vokabularju lahko dobimo v spremenljivki `model.wv.index_to_key`, vektorje pa z ukazom `model.wv[beseda]`. Sestavi slover iz prvih petih besed in pripadajočih vektorjev ter gesla in vrednosti v slovarju izpiši."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725eba6b29d0d617",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T09:37:06.423932Z",
     "start_time": "2023-11-16T09:37:06.409648Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c28edebb",
   "metadata": {},
   "source": [
    "2.c: Ukazom `model.wv.most_similar` in parametrom topn, lahko najdemo n najbolj podobnih besed znotraj slovarja. Izpiši prvih 10 najbližjih besed besedi \"cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26811258d5611c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:06:08.276810Z",
     "start_time": "2023-11-15T17:04:40.472606Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b87cb8c5",
   "metadata": {},
   "source": [
    "2.d: Model Word2Vec vsaki besedi priredi vektor. V naši nalogi delamo z besedili, ki so dolga več besed zato moramo te vektorje nekako zagregirat, na primer tako, da vektorje besed znotraj vsakega besedila povprečimo. Definiraj funkcijo, ki bo vsako besedilo iz učne in testne množice spremenila v vektor dolžine 100 (če v besedilu ni nobene besede naj bo to vektor ničel). Z dobljeno množico vektorjev preveri točnost logistične regresije za klasifikacijo iz naloge 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca1b774973aa6131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:06:08.277006Z",
     "start_time": "2023-11-15T17:04:40.539008Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0492233050a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:06:08.277448Z",
     "start_time": "2023-11-15T17:04:45.600037Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "772d0044e6d28390",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Naloga 3: Vnaprej naučeni modeli\n",
    "\n",
    "Zadnje čase se za procesiranje naravnega jezika uporabljajo predvsem vnaprej naučeni modeli. Vnaprej naučeni modeli so (ponavadi) velike nevronske mreže, ki so naučeni na veliki množici podatkov. Vnaprej naučene modele ponavadi uporabimo kot začetno točko, ki jo dotreniramo za našo nalogo. Ti modeli so uporabni iz več razlogov.\n",
    "\n",
    "1. **Generalizacija**: Vnaprej naučeni modeli so naučeni na velikih in raznolikih besedilnih korpusih, kar jim omogoča učenje posplošenih reprezentacij jezika. To jim omogoča, da se dokaj dobro izkažejo pri številnih nadaljnjih nalogah brez potrebe dotreniranja za posamezno nalogo.\n",
    "\n",
    "2. **Učinkovitost virov**: Uporaba Vnaprej naučenih modelov prihrani računalniške vire in čas. Namesto da bi uporabnik modele treniral od začetka, za kar potrebuje precej podatkov in računalniške moči, lahko izkoristi te vnaprej obstoječe, dobro trenirane modele.\n",
    "\n",
    "3. **Transfer learning**: Vnaprej naučeni modeli omogočajo, da znanje, pridobljeno pri eni nalogi, prenesemo na drugo sorodno nalogo. S dotreniranjem (finetuningom) na določenih naborih podatkov ali nalogah se lahko njihova zmogljivost znatno izboljša z minimalnim dodatnim učenjem.\n",
    "\n",
    "Dotreniranje se nanaša na postopek, pri katerem se vzame vnaprej naučen model in ga dotrenira na posebnem naboru podatkov ali nalogi. Posledično se njegovi parametri prilagodijo za boljše delovanje v tem posebnem kontekstu. Dotreniranje je pomembno saj:\n",
    "\n",
    "- **Prilagajanje na posamezno nalogo**: Dotreniranje omogoča modelu, da se prilagodi podatkov ali nalogi.\n",
    "\n",
    "- **Povečana zmogljivost**: Z dotreniranjem na podatkih, specifičnih za domeno ali nalogo, se lahko model nauči več značilnosti, specifičnih za nalogo, kar izboljša natančnost in učinkovitost za predvideno uporabo.\n",
    "\n",
    "- **Zmanjšana zahteva po podatkih**: Za dotreniranje modela je pogosto potrebnih manj podatkov kot za učenje modela od začetka. Če začnemo z vnaprej naučenim modelom, se lahko učinkovito učimo iz manjšega nabora podatkov, specifičnega za določeno področje, kar je koristno v scenarijih, kjer je na voljo omejena količina anotiranih podatkov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe122bc5ca65fc3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:06:08.277692Z",
     "start_time": "2023-11-15T17:04:59.863546Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "/home/sebastianmeznar/miniconda3/envs/ITAP24/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: POSITIVE, Confidence: 0.9999\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Inicializiramo model za analizo sentimenta\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "text = \"I absolutely love this product! It's fantastic!\"\n",
    "\n",
    "# Naredimo analizo sentimenta za zgornji vzorec\n",
    "result = sentiment_analysis(text)\n",
    "\n",
    "# Izpišemo sentiment vzorca in koliko je model \"prepričan\" v napoved\n",
    "print(f\"Sentiment: {result[0]['label']}, Confidence: {result[0]['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c99d17",
   "metadata": {},
   "source": [
    "3.a: Preveri, če se sentiment prvih 500 primerov sklada s ciljnimi vrednosti (torej, če model vrne \"POSITIVE\" je napovedana vrednost \"pos\", če ne \"neg\"). V model daj le prvih 1500 črk posameznega primera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84f74e7a503539",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T17:12:32.375177Z",
     "start_time": "2023-11-15T17:10:15.076512Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "trans_y_pred = []\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "n = 500\n",
    "\n",
    "for test_text in tqdm(X_test['text'][:n]):\n",
    "    pass\n",
    "\n",
    "trans_accuracy = accuracy_score(y_test_reset[:n], trans_y_pred)\n",
    "print(\"Transformer Accuracy:\", trans_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c7cbb0",
   "metadata": {},
   "source": [
    "## Dodatna naloga (v ang.): Stanza & POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a4bc0d8cd9989",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Stanza is an NLP library developed by the Stanford NLP Group. It's designed for a wide range of natural language processing tasks, including tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and more. Stanza aims to provide efficient and accurate pre-trained models for various languages.\n",
    "\n",
    "Key features of Stanza include:\n",
    "- **Pre-Trained Models**: Stanza comes with pre-trained models for multiple languages, allowing users to perform various NLP tasks without training models from scratch.\n",
    "- **Ease of Use**: It offers a simple and intuitive API for performing different NLP tasks, making it accessible for both beginners and experienced researchers.\n",
    "- **Accuracy**: Stanza models are known for their high accuracy in different NLP tasks due to their robust training on extensive datasets.\n",
    "- **Multiple Languages**: Stanza supports multiple languages, making it suitable for multilingual NLP applications.\n",
    "\n",
    "Stanza provides state-of-the-art performance in various NLP tasks and continues to evolve with advancements in the field of natural language processing.\n",
    "\n",
    "### Use Case: Text Analysis with Universal POS Tagging using Stanza\n",
    "\n",
    "Stanza's Universal POS tagging can be highly beneficial in various text analysis tasks. Let's consider a scenario where you have a dataset of customer reviews for a product. By utilizing Stanza's Universal POS tagging, you can perform the following analysis:\n",
    "\n",
    "1. **Extracting Key Features**: Identify the key features or attributes of the product mentioned in the reviews by analyzing nouns (NOUN) and adjectives (ADJ) tagged using Stanza. This helps in understanding what aspects of the product are being praised or criticized.\n",
    "\n",
    "2. **Sentiment Analysis**: Analyze sentiments associated with specific parts of speech. For instance, adjectives (ADJ) often reflect sentiments or opinions. By associating adjectives with their corresponding nouns, you can determine the sentiment expressed towards various product features.\n",
    "\n",
    "3. **Customer Feedback Categorization**: Categorize customer feedback into different categories based on the identified parts of speech. For instance, categorize reviews mentioning \"customer service\" (PROPN) separately to analyze the sentiment specifically related to that aspect.\n",
    "\n",
    "4. **Comparative Analysis**: Compare the frequency and sentiment of different parts of speech across different products or time frames to identify trends and patterns in customer opinions.\n",
    "\n",
    "By utilizing Stanza's Universal POS tagging, you can effectively extract meaningful insights from textual data, enabling better decision-making and improving products or services based on customer feedback.\n",
    "\n",
    "### Universal POS Tags\n",
    "- **ADJ**: Adjective\n",
    "- **ADP**: Adposition\n",
    "- **ADV**: Adverb\n",
    "- **AUX**: Auxiliary\n",
    "- **CCONJ**: Coordinating conjunction\n",
    "- **DET**: Determiner\n",
    "- **INTJ**: Interjection\n",
    "- **NOUN**: Noun\n",
    "- **NUM**: Numeral\n",
    "- **PART**: Particle\n",
    "- **PRON**: Pronoun\n",
    "- **PROPN**: Proper noun\n",
    "- **PUNCT**: Punctuation\n",
    "- **SCONJ**: Subordinating conjunction\n",
    "- **SYM**: Symbol\n",
    "- **VERB**: Verb\n",
    "- **X**: Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bab1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d729e48866f6451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T10:04:16.150895Z",
     "start_time": "2023-11-16T10:04:07.452456Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "# Download English model (change 'en' to the appropriate language code if needed)\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize the English pipeline\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,pos')\n",
    "\n",
    "# Sample customer review\n",
    "sample_review = \"The camera quality is amazing, but the battery life could be better.\"\n",
    "\n",
    "# Process the review\n",
    "doc = nlp(sample_review)\n",
    "\n",
    "# Extract nouns and adjectives\n",
    "nouns = []\n",
    "adjectives = []\n",
    "\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        if word.upos == 'NOUN':\n",
    "            nouns.append(word.text)\n",
    "        elif word.upos == 'ADJ':\n",
    "            adjectives.append(word.text)\n",
    "\n",
    "# Print extracted nouns and adjectives\n",
    "print(\"Extracted Nouns:\", nouns)\n",
    "print(\"Extracted Adjectives:\", adjectives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2af1b55",
   "metadata": {},
   "source": [
    "Prirejeno pa vajah Boshko-ta Koloskega (Inteligentni sistemi, FRI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
